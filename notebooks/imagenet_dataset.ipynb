{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/work/WorkSpace/GoogleUImageEmbed/.env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEmbed:\n",
    "\n",
    "    def __init__(self, category, path, embed):\n",
    "        self.category = category\n",
    "        self.path = path\n",
    "        self.embed = embed\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.path}, {self.embed}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_files(path):\n",
    "\n",
    "    path = Path(path)\n",
    "    print(path)\n",
    "\n",
    "    dirs = os.listdir(path)\n",
    "    print(\"Total Dirs:\", len(dirs))\n",
    "\n",
    "    all_files = []\n",
    "    total_files = 0\n",
    "    for category_name in dirs:\n",
    "        file_path_ls = list((path / category_name).glob(\"*.JPEG\"))\n",
    "        file_path_ls = [ImageEmbed(category_name, p, None) for p in  file_path_ls]\n",
    "        all_files.extend(file_path_ls)\n",
    "\n",
    "    print(\"Categories:\", len(dirs), \"- Files:\", len(all_files))\n",
    "\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/work/WorkSpace/dataset/imagenet-mini/train\n",
      "Total Dirs: 1000\n",
      "Categories: 1000 - Files: 34745\n",
      "/media/work/WorkSpace/dataset/imagenet-mini/val\n",
      "Total Dirs: 1000\n",
      "Categories: 1000 - Files: 3923\n"
     ]
    }
   ],
   "source": [
    "train_paths = get_list_of_files(\"/media/work/WorkSpace/dataset/imagenet-mini/train\")\n",
    "val_paths = get_list_of_files(\"/media/work/WorkSpace/dataset/imagenet-mini/val\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "!ls \"/media/work/WorkSpace/dataset/imagenet-mini/train/n02443484\" | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_paths = train_paths[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[/media/work/WorkSpace/dataset/imagenet-mini/train/n02443484/n02443484_15470.JPEG, None,\n",
       " /media/work/WorkSpace/dataset/imagenet-mini/train/n02443484/n02443484_10204.JPEG, None,\n",
       " /media/work/WorkSpace/dataset/imagenet-mini/train/n02443484/n02443484_11275.JPEG, None,\n",
       " /media/work/WorkSpace/dataset/imagenet-mini/train/n02443484/n02443484_11368.JPEG, None,\n",
       " /media/work/WorkSpace/dataset/imagenet-mini/train/n02443484/n02443484_12259.JPEG, None]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model loading.\n",
    "model = torch.jit.load('../models_pt/v10_swin_base_patch4_window7_224_in22k.pt')\n",
    "model.to('cuda')\n",
    "model.eval()\n",
    "embedding_fn = model\n",
    "\n",
    "def get_embedding(path):\n",
    "    # Load image and extract its embedding.\n",
    "    input_image = Image.open(path).convert(\"RGB\")\n",
    "    convert_to_tensor = transforms.Compose([transforms.PILToTensor()])\n",
    "    input_tensor = convert_to_tensor(input_image)\n",
    "    input_batch = input_tensor.unsqueeze(0).to('cuda')\n",
    "    with torch.no_grad():\n",
    "        embedding = torch.flatten(embedding_fn(input_batch)[0]).cpu().data.numpy()\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 3914/34745 [02:24<19:00, 27.03it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/media/work/WorkSpace/GoogleUImageEmbed/notebooks/imagenet_dataset.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/work/WorkSpace/GoogleUImageEmbed/notebooks/imagenet_dataset.ipynb#ch0000007?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(train_paths)):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/work/WorkSpace/GoogleUImageEmbed/notebooks/imagenet_dataset.ipynb#ch0000007?line=1'>2</a>\u001b[0m     v\u001b[39m.\u001b[39membed \u001b[39m=\u001b[39m get_embedding(v\u001b[39m.\u001b[39;49mpath)\n",
      "\u001b[1;32m/media/work/WorkSpace/GoogleUImageEmbed/notebooks/imagenet_dataset.ipynb Cell 9\u001b[0m in \u001b[0;36mget_embedding\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/work/WorkSpace/GoogleUImageEmbed/notebooks/imagenet_dataset.ipynb#ch0000007?line=11'>12</a>\u001b[0m input_batch \u001b[39m=\u001b[39m input_tensor\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/work/WorkSpace/GoogleUImageEmbed/notebooks/imagenet_dataset.ipynb#ch0000007?line=12'>13</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/work/WorkSpace/GoogleUImageEmbed/notebooks/imagenet_dataset.ipynb#ch0000007?line=13'>14</a>\u001b[0m     embedding \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(embedding_fn(input_batch)[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/work/WorkSpace/GoogleUImageEmbed/notebooks/imagenet_dataset.ipynb#ch0000007?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m embedding\n",
      "File \u001b[0;32m/media/work/WorkSpace/GoogleUImageEmbed/.env/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(tqdm(train_paths)):\n",
    "    v.embed = get_embedding(v.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_count = {}\n",
    "for v in train_paths:\n",
    "    c = category_count.get(v.category, 0)\n",
    "    category_count[v.category] = c + 1\n",
    "# category_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([v.embed for v in train_paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34745, 34745)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_mat = euclidean_distances(arr, arr)\n",
    "distance_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34745/34745 [01:25<00:00, 407.09it/s]\n"
     ]
    }
   ],
   "source": [
    "categry_distance = {}\n",
    "\n",
    "\n",
    "for i, v in enumerate(tqdm(train_paths)):\n",
    "    \n",
    "\n",
    "    distance = distance_mat[i]\n",
    "    indexes = np.argsort(distance)[:10]\n",
    "\n",
    "    result = [(train_paths[i].category, distance[i]) for i in indexes]\n",
    "\n",
    "    # skip self\n",
    "    result = result[1:]\n",
    "\n",
    "    min_len = min(5, category_count[v.category])\n",
    "\n",
    "    result = sum([int(r[0] == v.category) for r in result[:min_len]]) / min_len\n",
    "\n",
    "    rs_ls = categry_distance.get(v.category, [])\n",
    "    rs_ls.append(result)\n",
    "    categry_distance[v.category] = rs_ls\n",
    "\n",
    "    # print(result)\n",
    "\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1, 21997, 32099, 21587, 32092])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(distance_mat[1])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2670392385721423"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ls = []\n",
    "\n",
    "for k, v_ls in categry_distance.items():\n",
    "    score = sum(v_ls) / len(v_ls)\n",
    "    score_ls.append(score)\n",
    "\n",
    "sum(score_ls) / len(score_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v11_swin_base_patch4_window7_224_in22k.pt - 0.7354730610007483\n",
    "# 0.7473133925898999\n",
    "\n",
    "# 0.26703436052336177"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11c50a199cf0c6474eebc0ebac7d8e2b5b262f9cc868109f81680ce6f2cfa3b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
