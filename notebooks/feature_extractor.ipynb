{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    inception_model = models.inception_v3(pretrained=True)\n",
    "    inception_model.fc = nn.Linear(2048, 64)\n",
    "    self.feature_extractor = inception_model\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = transforms.functional.resize(x,size=[224, 224])\n",
    "    x = x/255.0\n",
    "    x = transforms.functional.normalize(x, \n",
    "                                            mean=[0.485, 0.456, 0.406], \n",
    "                                            std=[0.229, 0.224, 0.225])\n",
    "    return self.feature_extractor(x).logits\n",
    "\n",
    "model = MyModel()\n",
    "model.eval()\n",
    "saved_model = torch.jit.script(model)\n",
    "saved_model.save('../input/google-img-embed/inception_v3_saved_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [.logits] work only with torchscripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49236295 0.43916446 0.4789883  0.38781932 0.4920384  0.36499846\n",
      " 0.38398114 0.401796   0.5004146  0.5484579  0.3844925  0.50782156\n",
      " 0.627752   0.39378378 0.4953413  0.37244242 0.49768174 0.5064805\n",
      " 0.5457502  0.47064242 0.44930282 0.50932884 0.57070726 0.5515556\n",
      " 0.40232548 0.44075444 0.43162206 0.5554637  0.37993324 0.64674383\n",
      " 0.46709436 0.49590132 0.4667424  0.43552575 0.47120625 0.58384526\n",
      " 0.3633921  0.41693127 0.5534935  0.36043447 0.43745026 0.52090955\n",
      " 0.36178485 0.41308272 0.49650058 0.41861808 0.60746115 0.44994262\n",
      " 0.54319316 0.3947586  0.61874783 0.4871928  0.5517022  0.5635879\n",
      " 0.4268709  0.44035915 0.49573123 0.3042859  0.45980042 0.33129784\n",
      " 0.44358158 0.34073335 0.50487447 0.40105692]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Model loading.\n",
    "model = torch.jit.load('../input/google-img-embed/v4_resnet50_saved_model.pt')\n",
    "model.eval()\n",
    "embedding_fn = model\n",
    "\n",
    "# Load image and extract its embedding.\n",
    "input_image = Image.open('../input/images/dogs/collie-beach-bokeh.jpg').convert(\"RGB\")\n",
    "convert_to_tensor = transforms.Compose([transforms.PILToTensor()])\n",
    "input_tensor = convert_to_tensor(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "  embedding = torch.flatten(embedding_fn(input_batch)[0]).cpu().data.numpy()\n",
    "  print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11c50a199cf0c6474eebc0ebac7d8e2b5b262f9cc868109f81680ce6f2cfa3b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
