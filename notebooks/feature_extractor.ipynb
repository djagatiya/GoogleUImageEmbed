{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    inception_model = models.inception_v3(pretrained=True)\n",
    "    inception_model.fc = nn.Linear(2048, 64)\n",
    "    self.feature_extractor = inception_model\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = transforms.functional.resize(x,size=[224, 224])\n",
    "    x = x/255.0\n",
    "    x = transforms.functional.normalize(x, \n",
    "                                            mean=[0.485, 0.456, 0.406], \n",
    "                                            std=[0.229, 0.224, 0.225])\n",
    "    return self.feature_extractor(x).logits\n",
    "\n",
    "model = MyModel()\n",
    "model.eval()\n",
    "saved_model = torch.jit.script(model)\n",
    "saved_model.save('../input/google-img-embed/inception_v3_saved_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [.logits] work only with torchscripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.05671266  0.02757249 -0.1023538   0.024033   -0.17680858  0.3449868\n",
      " -0.4970118  -0.21332997 -0.25042892  0.08485723  0.315588    0.30897635\n",
      "  0.25909576 -0.07186266  0.09054497 -0.1933078   0.29890564  0.18681553\n",
      "  0.16552402 -0.34627232  0.13632624  0.01161262 -0.07296698  0.11399545\n",
      " -0.08872433 -0.25143853  0.0376635  -0.16649503 -0.06944361  0.13528076\n",
      "  0.29189467  0.04448745 -0.22423002 -0.30426562 -0.1743249  -0.1865472\n",
      " -0.15636072 -0.13407327 -0.29968047 -0.24464166  0.290393   -0.00605993\n",
      " -0.03487606  0.2089283   0.12893851  0.10404649  0.17819619 -0.06499936\n",
      "  0.01899411 -0.09067152 -0.08642957  0.04255173  0.12876575 -0.07403735\n",
      "  0.08794733  0.15112586 -0.09704171  0.06054981  0.1371095   0.179655\n",
      "  0.02621843  0.09447531  0.1027277  -0.15676223]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Model loading.\n",
    "model = torch.jit.load('../models_pt/v11_swin_base_patch4_window7_224_in22k.pt')\n",
    "model.eval()\n",
    "embedding_fn = model\n",
    "\n",
    "# Load image and extract its embedding.\n",
    "input_image = Image.open('../input/images/dogs/collie-beach-bokeh.jpg').convert(\"RGB\")\n",
    "convert_to_tensor = transforms.Compose([transforms.PILToTensor()])\n",
    "input_tensor = convert_to_tensor(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "  embedding = torch.flatten(embedding_fn(input_batch)[0]).cpu().data.numpy()\n",
    "  print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11c50a199cf0c6474eebc0ebac7d8e2b5b262f9cc868109f81680ce6f2cfa3b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
