{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    inception_model = models.inception_v3(pretrained=True)\n",
    "    inception_model.fc = nn.Linear(2048, 64)\n",
    "    self.feature_extractor = inception_model\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = transforms.functional.resize(x,size=[224, 224])\n",
    "    x = x/255.0\n",
    "    x = transforms.functional.normalize(x, \n",
    "                                            mean=[0.485, 0.456, 0.406], \n",
    "                                            std=[0.229, 0.224, 0.225])\n",
    "    return self.feature_extractor(x).logits\n",
    "\n",
    "model = MyModel()\n",
    "model.eval()\n",
    "saved_model = torch.jit.script(model)\n",
    "saved_model.save('../input/google-img-embed/inception_v3_saved_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [.logits] work only with torchscripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04315272 -0.0235291   0.0312604  -0.00041633  0.02815877  0.12055283\n",
      " -0.03224912 -0.01205595 -0.00194191  0.04195309  0.00266817 -0.08809998\n",
      " -0.10464027  0.02160696  0.07408638  0.04939732  0.03640577  0.00602856\n",
      "  0.01544393  0.04408798  0.19019389 -0.09772927 -0.0610893   0.00477735\n",
      " -0.10000484 -0.07538507  0.14449085 -0.04995918 -0.04938088  0.06414583\n",
      " -0.06592316 -0.07091008  0.10838479 -0.00762417 -0.09494048  0.10305584\n",
      "  0.01002913 -0.0395183   0.12190846  0.01152168  0.00955782  0.01028253\n",
      "  0.01631404 -0.00458292 -0.10308661  0.00719626 -0.00534671  0.07603404\n",
      " -0.02168461  0.02141615 -0.00448317  0.00851323 -0.02167099  0.06061072\n",
      "  0.13131109  0.17078684  0.04667639  0.06882825 -0.05217126  0.01565712\n",
      "  0.02451139 -0.00508972  0.02675567 -0.08019949]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Model loading.\n",
    "model = torch.jit.load('../models_pt/v7_swinv2_base_window16_256.pt')\n",
    "model.eval()\n",
    "embedding_fn = model\n",
    "\n",
    "# Load image and extract its embedding.\n",
    "input_image = Image.open('../input/images/dogs/collie-beach-bokeh.jpg').convert(\"RGB\")\n",
    "convert_to_tensor = transforms.Compose([transforms.PILToTensor()])\n",
    "input_tensor = convert_to_tensor(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "  embedding = torch.flatten(embedding_fn(input_batch)[0]).cpu().data.numpy()\n",
    "  print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11c50a199cf0c6474eebc0ebac7d8e2b5b262f9cc868109f81680ce6f2cfa3b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
