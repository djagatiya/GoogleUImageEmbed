{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    inception_model = models.inception_v3(pretrained=True)\n",
    "    inception_model.fc = nn.Linear(2048, 64)\n",
    "    self.feature_extractor = inception_model\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = transforms.functional.resize(x,size=[224, 224])\n",
    "    x = x/255.0\n",
    "    x = transforms.functional.normalize(x, \n",
    "                                            mean=[0.485, 0.456, 0.406], \n",
    "                                            std=[0.229, 0.224, 0.225])\n",
    "    return self.feature_extractor(x).logits\n",
    "\n",
    "model = MyModel()\n",
    "model.eval()\n",
    "saved_model = torch.jit.script(model)\n",
    "saved_model.save('../input/google-img-embed/inception_v3_saved_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [.logits] work only with torchscripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49913645 0.46906117 0.49825194 0.37195235 0.45691285 0.39204645\n",
      " 0.4079221  0.4142578  0.4886418  0.5636962  0.39359304 0.49995944\n",
      " 0.6120907  0.39657354 0.48164067 0.35197148 0.48297414 0.4926043\n",
      " 0.5482643  0.49669454 0.43279284 0.53634715 0.5341487  0.52608174\n",
      " 0.3791866  0.46428728 0.46451157 0.54932964 0.40856847 0.6544852\n",
      " 0.47272003 0.4910925  0.46992144 0.4340453  0.47640014 0.56555897\n",
      " 0.36480945 0.4114953  0.58823776 0.35343066 0.43007874 0.50794774\n",
      " 0.3404074  0.43273982 0.48107296 0.4425872  0.64142925 0.4329255\n",
      " 0.5477964  0.42309114 0.5992905  0.4794149  0.51749676 0.5859948\n",
      " 0.4350825  0.46696788 0.46880722 0.31757396 0.4422681  0.38940272\n",
      " 0.45007625 0.34040406 0.49756482 0.40075022]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Model loading.\n",
    "model = torch.jit.load('../input/google-img-embed/resnet50_saved_model.pt')\n",
    "model.eval()\n",
    "embedding_fn = model\n",
    "\n",
    "# Load image and extract its embedding.\n",
    "input_image = Image.open('../input/images/dogs/collie-beach-bokeh.jpg').convert(\"RGB\")\n",
    "convert_to_tensor = transforms.Compose([transforms.PILToTensor()])\n",
    "input_tensor = convert_to_tensor(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "  embedding = torch.flatten(embedding_fn(input_batch)[0]).cpu().data.numpy()\n",
    "  print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11c50a199cf0c6474eebc0ebac7d8e2b5b262f9cc868109f81680ce6f2cfa3b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
