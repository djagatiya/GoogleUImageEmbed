{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    inception_model = models.inception_v3(pretrained=True)\n",
    "    inception_model.fc = nn.Linear(2048, 64)\n",
    "    self.feature_extractor = inception_model\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = transforms.functional.resize(x,size=[224, 224])\n",
    "    x = x/255.0\n",
    "    x = transforms.functional.normalize(x, \n",
    "                                            mean=[0.485, 0.456, 0.406], \n",
    "                                            std=[0.229, 0.224, 0.225])\n",
    "    return self.feature_extractor(x).logits\n",
    "\n",
    "model = MyModel()\n",
    "model.eval()\n",
    "saved_model = torch.jit.script(model)\n",
    "saved_model.save('../input/google-img-embed/inception_v3_saved_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [.logits] work only with torchscripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00422927  0.07208674  0.0107396   0.02217594  0.00476109  0.01163914\n",
      " -0.00700159  0.11872683  0.01967308 -0.01705721  0.03650533  0.01849472\n",
      " -0.02746117 -0.02818933 -0.00114972 -0.07334472  0.08396803 -0.03251896\n",
      " -0.01345569 -0.04495557  0.01206339 -0.01713189  0.07153784  0.09358376\n",
      "  0.00809748  0.01467024 -0.05876682  0.04816409  0.0459237   0.12587154\n",
      " -0.0076489   0.03158112 -0.0370841   0.05056956  0.01578933  0.06055729\n",
      " -0.04742613  0.05835805  0.05572148  0.02805956  0.11293344  0.05028643\n",
      "  0.04443515 -0.0168138   0.07156266 -0.00231754  0.00723598 -0.00807834\n",
      "  0.01296688  0.07115047 -0.02048157 -0.02985774  0.03019468  0.01321307\n",
      " -0.0046351   0.01219096  0.0584572   0.04004542  0.03315777  0.02869853\n",
      "  0.03391372 -0.01115438  0.03653382  0.01278363]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Model loading.\n",
    "model = torch.jit.load('../input/google-img-embed/v6_efficientnet_b7.pt')\n",
    "model.eval()\n",
    "embedding_fn = model\n",
    "\n",
    "# Load image and extract its embedding.\n",
    "input_image = Image.open('../input/images/dogs/collie-beach-bokeh.jpg').convert(\"RGB\")\n",
    "convert_to_tensor = transforms.Compose([transforms.PILToTensor()])\n",
    "input_tensor = convert_to_tensor(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "  embedding = torch.flatten(embedding_fn(input_batch)[0]).cpu().data.numpy()\n",
    "  print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11c50a199cf0c6474eebc0ebac7d8e2b5b262f9cc868109f81680ce6f2cfa3b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
